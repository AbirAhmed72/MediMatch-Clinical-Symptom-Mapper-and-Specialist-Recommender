{"cells":[{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:30:54.615060Z","iopub.status.busy":"2023-11-07T03:30:54.613857Z","iopub.status.idle":"2023-11-07T03:31:08.654328Z","shell.execute_reply":"2023-11-07T03:31:08.652669Z","shell.execute_reply.started":"2023-11-07T03:30:54.615021Z"},"trusted":true},"outputs":[],"source":["# !pip install seqeval"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:31:08.658836Z","iopub.status.busy":"2023-11-07T03:31:08.658409Z","iopub.status.idle":"2023-11-07T03:31:08.667907Z","shell.execute_reply":"2023-11-07T03:31:08.666695Z","shell.execute_reply.started":"2023-11-07T03:31:08.658798Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import re\n","from transformers import AutoTokenizer\n","from datasets import Dataset, Features, Value, ClassLabel, Sequence\n","from seqeval.metrics import accuracy_score\n","# from seqeval.metrics import classification_report\n","# from seqeval.metrics import precision_score"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:31:08.670601Z","iopub.status.busy":"2023-11-07T03:31:08.670211Z","iopub.status.idle":"2023-11-07T03:31:08.911236Z","shell.execute_reply":"2023-11-07T03:31:08.909547Z","shell.execute_reply.started":"2023-11-07T03:31:08.670557Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>description</th>\n","      <th>medical_specialty</th>\n","      <th>sample_name</th>\n","      <th>transcription</th>\n","      <th>keywords</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A 23-year-old white female presents with comp...</td>\n","      <td>Allergy / Immunology</td>\n","      <td>Allergic Rhinitis</td>\n","      <td>SUBJECTIVE:,  This 23-year-old white female pr...</td>\n","      <td>allergy / immunology, allergic rhinitis, aller...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Consult for laparoscopic gastric bypass.</td>\n","      <td>Bariatrics</td>\n","      <td>Laparoscopic Gastric Bypass Consult - 2</td>\n","      <td>PAST MEDICAL HISTORY:, He has difficulty climb...</td>\n","      <td>bariatrics, laparoscopic gastric bypass, weigh...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Consult for laparoscopic gastric bypass.</td>\n","      <td>Bariatrics</td>\n","      <td>Laparoscopic Gastric Bypass Consult - 1</td>\n","      <td>HISTORY OF PRESENT ILLNESS: , I have seen ABC ...</td>\n","      <td>bariatrics, laparoscopic gastric bypass, heart...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2-D M-Mode. Doppler.</td>\n","      <td>Cardiovascular / Pulmonary</td>\n","      <td>2-D Echocardiogram - 1</td>\n","      <td>2-D M-MODE: , ,1.  Left atrial enlargement wit...</td>\n","      <td>cardiovascular / pulmonary, 2-d m-mode, dopple...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2-D Echocardiogram</td>\n","      <td>Cardiovascular / Pulmonary</td>\n","      <td>2-D Echocardiogram - 2</td>\n","      <td>1.  The left ventricular cavity size and wall ...</td>\n","      <td>cardiovascular / pulmonary, 2-d, doppler, echo...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                         description  \\\n","0   A 23-year-old white female presents with comp...   \n","1           Consult for laparoscopic gastric bypass.   \n","2           Consult for laparoscopic gastric bypass.   \n","3                             2-D M-Mode. Doppler.     \n","4                                 2-D Echocardiogram   \n","\n","             medical_specialty                                sample_name  \\\n","0         Allergy / Immunology                         Allergic Rhinitis    \n","1                   Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n","2                   Bariatrics   Laparoscopic Gastric Bypass Consult - 1    \n","3   Cardiovascular / Pulmonary                    2-D Echocardiogram - 1    \n","4   Cardiovascular / Pulmonary                    2-D Echocardiogram - 2    \n","\n","                                       transcription  \\\n","0  SUBJECTIVE:,  This 23-year-old white female pr...   \n","1  PAST MEDICAL HISTORY:, He has difficulty climb...   \n","2  HISTORY OF PRESENT ILLNESS: , I have seen ABC ...   \n","3  2-D M-MODE: , ,1.  Left atrial enlargement wit...   \n","4  1.  The left ventricular cavity size and wall ...   \n","\n","                                            keywords  \n","0  allergy / immunology, allergic rhinitis, aller...  \n","1  bariatrics, laparoscopic gastric bypass, weigh...  \n","2  bariatrics, laparoscopic gastric bypass, heart...  \n","3  cardiovascular / pulmonary, 2-d m-mode, dopple...  \n","4  cardiovascular / pulmonary, 2-d, doppler, echo...  "]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv('../datasets/mtsamples.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Required Functions"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:31:08.914830Z","iopub.status.busy":"2023-11-07T03:31:08.914422Z","iopub.status.idle":"2023-11-07T03:31:08.946410Z","shell.execute_reply":"2023-11-07T03:31:08.944837Z","shell.execute_reply.started":"2023-11-07T03:31:08.914797Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["def get_transcription_types():\n","    sub_ls = []\n","    for ts in df['transcription']:\n","        data = get_initials(ts)\n","        if data is not None:\n","            sub_ls.append(data)\n","\n","\n","    unique_sub_ls = tuple()\n","    for sub in sub_ls:\n","        if unique_sub_ls.count(sub) == 0:\n","            unique_sub_ls = unique_sub_ls + (sub,)\n","\n","    return sorted(unique_sub_ls)\n","\n","\n","def get_initials(data):\n","    data = str(data)\n","    subject = ''\n","    subject = data.split(',')\n","    if len(subject[0]) < 50:\n","        subject[0] = subject[0].strip()\n","        words = subject[0].split(' ')\n","        if words[0] == words[0].upper():\n","            if subject[0][-1] == ':':\n","                subject[0] = subject[0][:-1]\n","                return subject[0]\n","    \n","\n","def get_tokens_with_entities(raw_text: str):\n","    # split the text by spaces only if the space does not occur between square brackets\n","    # we do not want to split \"multi-word\" entity value yet\n","    raw_tokens = re.split(r\"\\s(?![^\\[]*\\])\", raw_text)\n","\n","    # a regex for matching the annotation according to our notation [entity_value](entity_name)\n","    entity_value_pattern = r\"\\[(?P<value>.+?)\\]\\((?P<entity>.+?)\\)\"\n","    entity_value_pattern_compiled = re.compile(entity_value_pattern, flags=re.I|re.M)\n","\n","    tokens_with_entities = []\n","\n","    for raw_token in raw_tokens:\n","        match = entity_value_pattern_compiled.match(raw_token)\n","        if match:\n","            raw_entity_name, raw_entity_value = match.group(\"entity\"), match.group(\"value\")\n","\n","            # we prefix the name of entity differently\n","            # B- indicates beginning of an entity\n","            # I- indicates the token is not a new entity itself but rather a part of existing one\n","            for i, raw_entity_token in enumerate(re.split(\"\\s\", raw_entity_value)):\n","                entity_prefix = \"B\" if i == 0 else \"I\"\n","                entity_name = f\"{entity_prefix}-{raw_entity_name}\"\n","                tokens_with_entities.append((raw_entity_token, entity_name))\n","        else:\n","            tokens_with_entities.append((raw_token, \"O\"))\n","\n","    return tokens_with_entities\n","\n","def remove_symptom_notation(text):\n","    # Remove [symptom] notation\n","    text = re.sub(r'\\[(.*?)\\]', r'\\1', text)\n","    # Remove (symptom) notation\n","    text = re.sub(r'\\((.*?)\\)', r'\\1', text)\n","    # Remove the word \"symptom\"\n","    text = text.replace('symptom', '')\n","    if not text[-1].isalpha(): text = text[:-1]\n","    return text\n","\n","def set_token_entities(text, predicted_text):\n","    \n","    symptom_pos = []\n","    for entity in predicted_text:\n","        symptom_pos.append((entity['start'], entity['end']))\n","    \n","    entity_set = []\n","    done = []\n","    \n","    for coord in symptom_pos:\n","        extracted_text = text[coord[0]: coord[1]]\n","        \n","        words = extracted_text.split(' ')\n","        if len(words) == 1:\n","            entity_set.append((words[0], 'B-symptom'))\n","            done.append(words[0])\n","        else:\n","            for i in range(len(words)):\n","                if i == 0: \n","                    entity_set.append((words[i], 'B-symptom'))\n","                else:\n","                    entity_set.append((words[i], 'I-symptom'))\n","                done.append(words[i])\n","                \n","    words = text.split(' ')\n","    for i in range(len(words)):\n","        if words[i] not in done:\n","            entity_set.append((words[i], 'O'))\n","            \n","    # reorder list \n","    final_output = []\n","    for word in words:\n","        for item in entity_set:\n","            if word == item[0]:\n","                final_output.append(item)\n","                break\n","    \n","    return final_output\n","\n","def evaluate(y_true, y_pred, verbose):\n","    \n","    a = []\n","    b = []\n","    for tple in y_true:\n","        a.append(tple[1])\n","        \n","    for tple in y_pred:\n","        b.append(tple[1])\n","    \n","    if verbose:\n","        print(\"Accuracy: \", round(accuracy_score([a], [b]), 2))\n","        print(\"===\\n\")\n","        \n","    return round(accuracy_score([a], [b]), 2)\n","    \n","    \n","print('done')"]},{"cell_type":"markdown","metadata":{},"source":["### Exploring the Dataset\n","**It's necessary as we want to find out the appropiate data points that are suitable to train our model**"]},{"cell_type":"code","execution_count":19,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-11-07T03:31:08.948378Z","iopub.status.busy":"2023-11-07T03:31:08.947948Z","iopub.status.idle":"2023-11-07T03:31:09.415440Z","shell.execute_reply":"2023-11-07T03:31:09.414261Z","shell.execute_reply.started":"2023-11-07T03:31:08.948345Z"},"trusted":true},"outputs":[],"source":["# finding the counts of each of transcription types\n","ts_type = get_transcription_types()\n","ts_type_count = {}\n","\n","for item in ts_type:\n","    ts_type_count[item] = 0\n","        \n","for idx, ndf in df.iterrows():\n","    data = ndf['transcription']\n","    initial = get_initials(data)\n","    if initial is not None:\n","        ts_type_count[str(initial)] += 1\n","\n","sorted_ts_count = dict(sorted(ts_type_count.items(), key=lambda item: item[0], reverse=False))\n","# for ts, cnt in sorted_ts_count.items():\n","#     print(ts, cnt)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:31:09.416993Z","iopub.status.busy":"2023-11-07T03:31:09.416679Z","iopub.status.idle":"2023-11-07T03:31:09.824099Z","shell.execute_reply":"2023-11-07T03:31:09.823050Z","shell.execute_reply.started":"2023-11-07T03:31:09.416966Z"},"trusted":true},"outputs":[],"source":["with open('complain.txt', 'w+') as f:\n","    # save only the subjective transciptions\n","    for idx, ndf in df.iterrows():\n","        data = ndf['transcription']\n","        initial = get_initials(data)\n","        if initial == 'SUBJECTIVE':\n","            transcript = ndf['transcription']\n","            parts = transcript.split(':,')\n","#             print(transcript)\n","            f.write(transcript)\n","            f.write('\\n\\n')\n","\n","#             print('>>', end=' ')\n","#             print(ndf['keywords'])\n","#             print('--\\n')\n","            \n","    f.close"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset Maker Function for Training with Transformers"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:31:09.826576Z","iopub.status.busy":"2023-11-07T03:31:09.825774Z","iopub.status.idle":"2023-11-07T03:31:09.853156Z","shell.execute_reply":"2023-11-07T03:31:09.851860Z","shell.execute_reply.started":"2023-11-07T03:31:09.826544Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["class NERDataMaker:\n","    def __init__(self, texts):\n","        self.unique_entities = []\n","        self.processed_texts = []\n","\n","        temp_processed_texts = []\n","        for text in texts:\n","            tokens_with_entities = get_tokens_with_entities(text)\n","            for _, ent in tokens_with_entities:\n","                if ent not in self.unique_entities:\n","                    self.unique_entities.append(ent)\n","            temp_processed_texts.append(tokens_with_entities)\n","\n","        self.unique_entities.sort(key=lambda ent: ent if ent != \"O\" else \"\")\n","\n","        for tokens_with_entities in temp_processed_texts:\n","            self.processed_texts.append([(t, self.unique_entities.index(ent)) for t, ent in tokens_with_entities])\n","\n","    @property\n","    def id2label(self):\n","        return dict(enumerate(self.unique_entities))\n","\n","    @property\n","    def label2id(self):\n","        return {v:k for k, v in self.id2label.items()}\n","\n","    def __len__(self):\n","        return len(self.processed_texts)\n","\n","    def __getitem__(self, idx):\n","        def _process_tokens_for_one_text(id, tokens_with_encoded_entities):\n","            ner_tags = []\n","            tokens = []\n","            for t, ent in tokens_with_encoded_entities:\n","                ner_tags.append(ent)\n","                tokens.append(t)\n","\n","            return {\n","                \"id\": id,\n","                \"ner_tags\": ner_tags,\n","                \"tokens\": tokens\n","            }\n","\n","        tokens_with_encoded_entities = self.processed_texts[idx]\n","        if isinstance(idx, int):\n","            return _process_tokens_for_one_text(idx, tokens_with_encoded_entities)\n","        else:\n","            return [_process_tokens_for_one_text(i+idx.start, tee) for i, tee in enumerate(tokens_with_encoded_entities)]\n","\n","    def as_hf_dataset(self, tokenizer):\n","        \n","        def tokenize_and_align_labels(examples):\n","            tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n","\n","            labels = []\n","            for i, label in enumerate(examples[f\"ner_tags\"]):\n","                word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n","                previous_word_idx = None\n","                label_ids = []\n","                for word_idx in word_ids:  # Set the special tokens to -100.\n","                    if word_idx is None:\n","                        label_ids.append(-100)\n","                    elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n","                        label_ids.append(label[word_idx])\n","                    else:\n","                        label_ids.append(-100)\n","                    previous_word_idx = word_idx\n","                labels.append(label_ids)\n","\n","            tokenized_inputs[\"labels\"] = labels\n","            return tokenized_inputs\n","\n","        ids, ner_tags, tokens = [], [], []\n","        for i, pt in enumerate(self.processed_texts):\n","            ids.append(i)\n","            pt_tokens,pt_tags = list(zip(*pt))\n","            ner_tags.append(pt_tags)\n","            tokens.append(pt_tokens)\n","        data = {\n","            \"id\": ids,\n","            \"ner_tags\": ner_tags,\n","            \"tokens\": tokens\n","        }\n","        features = Features({\n","            \"tokens\": Sequence(Value(\"string\")),\n","            \"ner_tags\": Sequence(ClassLabel(names=self.unique_entities)),\n","            \"id\": Value(\"int32\")\n","        })\n","        print(\"Features: \", features)\n","        ds = Dataset.from_dict(data, features)\n","        tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)\n","        return tokenized_ds\n","    \n","print('done')"]},{"cell_type":"markdown","metadata":{},"source":["### Reading Data "]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:31:09.855138Z","iopub.status.busy":"2023-11-07T03:31:09.854811Z","iopub.status.idle":"2023-11-07T03:31:09.867878Z","shell.execute_reply":"2023-11-07T03:31:09.866204Z","shell.execute_reply.started":"2023-11-07T03:31:09.855111Z"},"trusted":true},"outputs":[],"source":["data = \"\"\n","with open('../datasets/annotated.txt', 'r') as f:\n","    lines = f.readlines()\n","    for line in lines:\n","        data += line"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:31:09.869757Z","iopub.status.busy":"2023-11-07T03:31:09.869337Z","iopub.status.idle":"2023-11-07T03:31:09.894118Z","shell.execute_reply":"2023-11-07T03:31:09.892994Z","shell.execute_reply.started":"2023-11-07T03:31:09.869727Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["275\n","220 55\n"]}],"source":["import random\n","\n","temp = \"\"\"1. This 35-year-old male patient comes in with complaints of [headache](symptom) and [nausea](symptom) after a recent head injury.\n","2. A 28-year-old female reports [shortness of breath](symptom), [chest pain](symptom), and [palpitations](symptom) during physical activity.\n","3. The 42-year-old patient describes [fever](symptom), [cough](symptom), and [fatigue](symptom) as their main symptoms.\n","4. This 19-year-old student presents with [fever](symptom), [sore throat](symptom), and [loss of taste and smell](symptom) for the past few days.\n","5. A 50-year-old male patient experiences [joint pain](symptom), [muscle weakness](symptom), and [fatigue](symptom).\n","6. The 31-year-old athlete complains of [knee pain](symptom), [swelling](symptom), and [limited range of motion](symptom) after a sports injury.\n","7. This 60-year-old woman reports [back pain](symptom), [numbness in legs](symptom), and [weakness](symptom) in her lower limbs.\n","8. A 25-year-old individual presents with [abdominal pain](symptom), [bloating](symptom), and [constipation](symptom).\n","9. The 48-year-old patient describes [vision problems](symptom), [headache](symptom), and [dizziness](symptom) for the past week.\n","10. This 22-year-old male reports [skin rash](symptom), [itching](symptom), and [redness](symptom) on various parts of the body.\"\"\"\n","\n","lines = data.strip().split('\\n')\n","random.shuffle(lines)\n","print(len(lines))\n","\n","train_ratio = 0.8  # 80% for training, 20% for testing\n","\n","# Calculate the split point based on the ratio\n","split_point = int(len(lines) * train_ratio)\n","\n","train_lines = lines[:split_point]\n","test_lines = lines[split_point:]\n","\n","train_text = '\\n'.join(train_lines)\n","test_text = '\\n'.join(test_lines)\n","\n","train_dm = NERDataMaker(train_text.split('\\n'))\n","test_dm = NERDataMaker(test_text.split('\\n'))\n","\n","print(len(train_dm), len(test_dm))"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:31:09.897441Z","iopub.status.busy":"2023-11-07T03:31:09.897110Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['O', 'B-symptom', 'I-symptom']\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Features:  {'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-symptom', 'I-symptom'], id=None), length=-1, id=None), 'id': Value(dtype='int32', id=None)}\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 220/220 [00:00<00:00, 7115.13 examples/s]\n"]},{"name":"stdout","output_type":"stream","text":["Features:  {'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-symptom', 'I-symptom'], id=None), length=-1, id=None), 'id': Value(dtype='int32', id=None)}\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 55/55 [00:00<00:00, 4844.12 examples/s]\n"]},{"name":"stdout","output_type":"stream","text":["{'id': 0, 'ner_tags': [0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['The', '38-year-old', 'female', 'patient', 'with', 'gestational', 'diabetes', 'reports', 'being', 'scared', 'to', 'eat', 'due', 'to', 'its', 'impact', 'on', 'her', 'blood', 'sugars.'], 'input_ids': [101, 1996, 4229, 1011, 2095, 1011, 2214, 2931, 5776, 2007, 16216, 20100, 2389, 14671, 4311, 2108, 6015, 2000, 4521, 2349, 2000, 2049, 4254, 2006, 2014, 2668, 5699, 2015, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, -100, -100, -100, -100, 0, 0, 0, 1, -100, -100, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, -100]}\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/154 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","  9%|▉         | 14/154 [00:17<02:37,  1.12s/it]\n","  9%|▉         | 14/154 [00:18<02:37,  1.12s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.5543782114982605, 'eval_runtime': 1.1424, 'eval_samples_per_second': 48.142, 'eval_steps_per_second': 3.501, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 18%|█▊        | 28/154 [00:36<02:40,  1.28s/it]\n"," 18%|█▊        | 28/154 [00:37<02:40,  1.28s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.43235722184181213, 'eval_runtime': 1.1357, 'eval_samples_per_second': 48.427, 'eval_steps_per_second': 3.522, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 42/154 [00:53<02:02,  1.09s/it]\n"," 27%|██▋       | 42/154 [00:55<02:02,  1.09s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.3724110722541809, 'eval_runtime': 1.2864, 'eval_samples_per_second': 42.755, 'eval_steps_per_second': 3.109, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▋      | 56/154 [01:12<01:58,  1.21s/it]\n"," 36%|███▋      | 56/154 [01:13<01:58,  1.21s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.33997854590415955, 'eval_runtime': 0.9982, 'eval_samples_per_second': 55.1, 'eval_steps_per_second': 4.007, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 45%|████▌     | 70/154 [01:31<01:43,  1.23s/it]\n"," 45%|████▌     | 70/154 [01:32<01:43,  1.23s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.3083362579345703, 'eval_runtime': 1.0953, 'eval_samples_per_second': 50.215, 'eval_steps_per_second': 3.652, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 55%|█████▍    | 84/154 [01:49<01:21,  1.17s/it]\n"," 55%|█████▍    | 84/154 [01:50<01:21,  1.17s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.299347847700119, 'eval_runtime': 1.1642, 'eval_samples_per_second': 47.243, 'eval_steps_per_second': 3.436, 'epoch': 6.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 64%|██████▎   | 98/154 [02:11<01:42,  1.83s/it]\n"," 64%|██████▎   | 98/154 [02:13<01:42,  1.83s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.30028873682022095, 'eval_runtime': 1.4, 'eval_samples_per_second': 39.286, 'eval_steps_per_second': 2.857, 'epoch': 7.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 73%|███████▎  | 112/154 [02:41<01:17,  1.85s/it]\n"," 73%|███████▎  | 112/154 [02:44<01:17,  1.85s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.29486194252967834, 'eval_runtime': 2.9355, 'eval_samples_per_second': 18.736, 'eval_steps_per_second': 1.363, 'epoch': 8.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 82%|████████▏ | 126/154 [03:15<00:57,  2.05s/it]\n"," 82%|████████▏ | 126/154 [03:17<00:57,  2.05s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.3016219735145569, 'eval_runtime': 1.6742, 'eval_samples_per_second': 32.852, 'eval_steps_per_second': 2.389, 'epoch': 9.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 91%|█████████ | 140/154 [03:40<00:19,  1.39s/it]\n"," 91%|█████████ | 140/154 [03:41<00:19,  1.39s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.29943332076072693, 'eval_runtime': 1.2605, 'eval_samples_per_second': 43.632, 'eval_steps_per_second': 3.173, 'epoch': 10.0}\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 154/154 [04:01<00:00,  1.37s/it]\n","100%|██████████| 154/154 [04:03<00:00,  1.58s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.30053454637527466, 'eval_runtime': 1.4373, 'eval_samples_per_second': 38.267, 'eval_steps_per_second': 2.783, 'epoch': 11.0}\n","{'train_runtime': 243.2281, 'train_samples_per_second': 9.95, 'train_steps_per_second': 0.633, 'train_loss': 0.2645307763830408, 'epoch': 11.0}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/plain":["TrainOutput(global_step=154, training_loss=0.2645307763830408, metrics={'train_runtime': 243.2281, 'train_samples_per_second': 9.95, 'train_steps_per_second': 0.633, 'train_loss': 0.2645307763830408, 'epoch': 11.0})"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n","\n","print(train_dm.unique_entities)\n","\n","model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(train_dm.unique_entities), id2label=train_dm.id2label, label2id=train_dm.label2id)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,             # keep it 2e-5\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=11,           # 11 epoch gives optimal validation loss\n","    weight_decay=0.01,\n",")\n","\n","train_ds = train_dm.as_hf_dataset(tokenizer=tokenizer)\n","test_ds = test_dm.as_hf_dataset(tokenizer=tokenizer)\n","\n","print(train_ds[0])\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_ds,\n","    eval_dataset=test_ds, # eval on training set! ONLY for DEMO!! have to split the data into train test split\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")\n","\n","trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["**Issue is the valuation loss became very high at the current tuned model, We have to tune further**"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["('./fine_tuned_model/tokenizer_config.json',\n"," './fine_tuned_model/special_tokens_map.json',\n"," './fine_tuned_model/vocab.txt',\n"," './fine_tuned_model/added_tokens.json',\n"," './fine_tuned_model/tokenizer.json')"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["model.save_pretrained(\"./fine_tuned_model\")\n","tokenizer.save_pretrained(\"./fine_tuned_model\")"]},{"cell_type":"code","execution_count":26,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 4/4 [00:00<00:00,  4.16it/s]"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.301\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from datasets import load_metric\n","metric = load_metric(\"accuracy\")\n","\n","# Evaluate the model on the test dataset\n","results = trainer.evaluate()\n","\n","# Calculate accuracy\n","accuracy = results\n","print(f\"Validation Loss: {round(accuracy['eval_loss'], 3)}\")"]},{"cell_type":"code","execution_count":27,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Average Accuracy:  80.9 %\n"]}],"source":["# Evaluating the model\n","from transformers import pipeline\n","\n","pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\") # pass device=0 if using gpu\n","\n","test_data = test_text.split('\\n')\n","avg_acc = []\n","for data in test_data:\n","    \n","    y_true = get_tokens_with_entities(data)\n","    \n","    data = remove_symptom_notation(data)\n","    predicted_text = pipe(data)\n","    \n","    y_pred = set_token_entities(data, predicted_text)\n","    \n","    avg_acc.append(evaluate(y_true, y_pred, False))\n","    \n","print(\"Average Accuracy: \", round(sum(avg_acc)/len(avg_acc), 3)*100, '%')"]},{"cell_type":"code","execution_count":29,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Symptom:  increased\n","Symptom:  thirst\n","Symptom:  hunger\n","Symptom:  frequent\n","Symptom:  urination\n","Symptom:  fatigue\n","Symptom:  blurred vision\n","Symptom:  slow\n","Symptom:  healing wounds\n","Symptom:  numbness\n","Symptom:  tingling in the hands and feet\n"]}],"source":["from transformers import pipeline\n","\n","pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\") # pass device=0 if using gpu\n","data = '''\n","The common symptoms of diabetes include increased thirst and hunger, frequent urination, fatigue, blurred vision, slow-healing wounds, and numbness or tingling in the hands and feet\n","'''\n","\n","data = remove_symptom_notation(data)\n","predicted_text = pipe(data)\n","for i in range(len(predicted_text)):\n","    print('Symptom: ', predicted_text[i]['word'])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
