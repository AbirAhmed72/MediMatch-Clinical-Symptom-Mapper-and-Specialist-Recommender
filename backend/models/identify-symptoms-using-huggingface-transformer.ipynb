{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:30:54.615060Z","iopub.status.busy":"2023-11-07T03:30:54.613857Z","iopub.status.idle":"2023-11-07T03:31:08.654328Z","shell.execute_reply":"2023-11-07T03:31:08.652669Z","shell.execute_reply.started":"2023-11-07T03:30:54.615021Z"},"trusted":true},"outputs":[],"source":["# !pip install seqeval"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:31:08.658836Z","iopub.status.busy":"2023-11-07T03:31:08.658409Z","iopub.status.idle":"2023-11-07T03:31:08.667907Z","shell.execute_reply":"2023-11-07T03:31:08.666695Z","shell.execute_reply.started":"2023-11-07T03:31:08.658798Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/fahad/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import pandas as pd\n","import re\n","from transformers import AutoTokenizer\n","from datasets import Dataset, Features, Value, ClassLabel, Sequence\n","from seqeval.metrics import accuracy_score\n","# from seqeval.metrics import classification_report\n","# from seqeval.metrics import precision_score"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:31:08.670601Z","iopub.status.busy":"2023-11-07T03:31:08.670211Z","iopub.status.idle":"2023-11-07T03:31:08.911236Z","shell.execute_reply":"2023-11-07T03:31:08.909547Z","shell.execute_reply.started":"2023-11-07T03:31:08.670557Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>description</th>\n","      <th>medical_specialty</th>\n","      <th>sample_name</th>\n","      <th>transcription</th>\n","      <th>keywords</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A 23-year-old white female presents with comp...</td>\n","      <td>Allergy / Immunology</td>\n","      <td>Allergic Rhinitis</td>\n","      <td>SUBJECTIVE:,  This 23-year-old white female pr...</td>\n","      <td>allergy / immunology, allergic rhinitis, aller...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Consult for laparoscopic gastric bypass.</td>\n","      <td>Bariatrics</td>\n","      <td>Laparoscopic Gastric Bypass Consult - 2</td>\n","      <td>PAST MEDICAL HISTORY:, He has difficulty climb...</td>\n","      <td>bariatrics, laparoscopic gastric bypass, weigh...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Consult for laparoscopic gastric bypass.</td>\n","      <td>Bariatrics</td>\n","      <td>Laparoscopic Gastric Bypass Consult - 1</td>\n","      <td>HISTORY OF PRESENT ILLNESS: , I have seen ABC ...</td>\n","      <td>bariatrics, laparoscopic gastric bypass, heart...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2-D M-Mode. Doppler.</td>\n","      <td>Cardiovascular / Pulmonary</td>\n","      <td>2-D Echocardiogram - 1</td>\n","      <td>2-D M-MODE: , ,1.  Left atrial enlargement wit...</td>\n","      <td>cardiovascular / pulmonary, 2-d m-mode, dopple...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2-D Echocardiogram</td>\n","      <td>Cardiovascular / Pulmonary</td>\n","      <td>2-D Echocardiogram - 2</td>\n","      <td>1.  The left ventricular cavity size and wall ...</td>\n","      <td>cardiovascular / pulmonary, 2-d, doppler, echo...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                         description  \\\n","0   A 23-year-old white female presents with comp...   \n","1           Consult for laparoscopic gastric bypass.   \n","2           Consult for laparoscopic gastric bypass.   \n","3                             2-D M-Mode. Doppler.     \n","4                                 2-D Echocardiogram   \n","\n","             medical_specialty                                sample_name  \\\n","0         Allergy / Immunology                         Allergic Rhinitis    \n","1                   Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n","2                   Bariatrics   Laparoscopic Gastric Bypass Consult - 1    \n","3   Cardiovascular / Pulmonary                    2-D Echocardiogram - 1    \n","4   Cardiovascular / Pulmonary                    2-D Echocardiogram - 2    \n","\n","                                       transcription  \\\n","0  SUBJECTIVE:,  This 23-year-old white female pr...   \n","1  PAST MEDICAL HISTORY:, He has difficulty climb...   \n","2  HISTORY OF PRESENT ILLNESS: , I have seen ABC ...   \n","3  2-D M-MODE: , ,1.  Left atrial enlargement wit...   \n","4  1.  The left ventricular cavity size and wall ...   \n","\n","                                            keywords  \n","0  allergy / immunology, allergic rhinitis, aller...  \n","1  bariatrics, laparoscopic gastric bypass, weigh...  \n","2  bariatrics, laparoscopic gastric bypass, heart...  \n","3  cardiovascular / pulmonary, 2-d m-mode, dopple...  \n","4  cardiovascular / pulmonary, 2-d, doppler, echo...  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv('../datasets/mtsamples.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Required Functions"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:31:08.914830Z","iopub.status.busy":"2023-11-07T03:31:08.914422Z","iopub.status.idle":"2023-11-07T03:31:08.946410Z","shell.execute_reply":"2023-11-07T03:31:08.944837Z","shell.execute_reply.started":"2023-11-07T03:31:08.914797Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["def get_transcription_types():\n","    sub_ls = []\n","    for ts in df['transcription']:\n","        data = get_initials(ts)\n","        if data is not None:\n","            sub_ls.append(data)\n","\n","\n","    unique_sub_ls = tuple()\n","    for sub in sub_ls:\n","        if unique_sub_ls.count(sub) == 0:\n","            unique_sub_ls = unique_sub_ls + (sub,)\n","\n","    return sorted(unique_sub_ls)\n","\n","\n","def get_initials(data):\n","    data = str(data)\n","    subject = ''\n","    subject = data.split(',')\n","    if len(subject[0]) < 50:\n","        subject[0] = subject[0].strip()\n","        words = subject[0].split(' ')\n","        if words[0] == words[0].upper():\n","            if subject[0][-1] == ':':\n","                subject[0] = subject[0][:-1]\n","                return subject[0]\n","    \n","\n","def get_tokens_with_entities(raw_text: str):\n","    # split the text by spaces only if the space does not occur between square brackets\n","    # we do not want to split \"multi-word\" entity value yet\n","    raw_tokens = re.split(r\"\\s(?![^\\[]*\\])\", raw_text)\n","\n","    # a regex for matching the annotation according to our notation [entity_value](entity_name)\n","    entity_value_pattern = r\"\\[(?P<value>.+?)\\]\\((?P<entity>.+?)\\)\"\n","    entity_value_pattern_compiled = re.compile(entity_value_pattern, flags=re.I|re.M)\n","\n","    tokens_with_entities = []\n","\n","    for raw_token in raw_tokens:\n","        match = entity_value_pattern_compiled.match(raw_token)\n","        if match:\n","            raw_entity_name, raw_entity_value = match.group(\"entity\"), match.group(\"value\")\n","\n","            # we prefix the name of entity differently\n","            # B- indicates beginning of an entity\n","            # I- indicates the token is not a new entity itself but rather a part of existing one\n","            for i, raw_entity_token in enumerate(re.split(\"\\s\", raw_entity_value)):\n","                entity_prefix = \"B\" if i == 0 else \"I\"\n","                entity_name = f\"{entity_prefix}-{raw_entity_name}\"\n","                tokens_with_entities.append((raw_entity_token, entity_name))\n","        else:\n","            tokens_with_entities.append((raw_token, \"O\"))\n","\n","    return tokens_with_entities\n","\n","def remove_symptom_notation(text):\n","    # Remove [symptom] notation\n","    text = re.sub(r'\\[(.*?)\\]', r'\\1', text)\n","    # Remove (symptom) notation\n","    text = re.sub(r'\\((.*?)\\)', r'\\1', text)\n","    # Remove the word \"symptom\"\n","    text = text.replace('symptom', '')\n","    if not text[-1].isalpha(): text = text[:-1]\n","    return text\n","\n","def set_token_entities(text, predicted_text):\n","    \n","    symptom_pos = []\n","    for entity in predicted_text:\n","        symptom_pos.append((entity['start'], entity['end']))\n","    \n","    entity_set = []\n","    done = []\n","    \n","    for coord in symptom_pos:\n","        extracted_text = text[coord[0]: coord[1]]\n","        \n","        words = extracted_text.split(' ')\n","        if len(words) == 1:\n","            entity_set.append((words[0], 'B-symptom'))\n","            done.append(words[0])\n","        else:\n","            for i in range(len(words)):\n","                if i == 0: \n","                    entity_set.append((words[i], 'B-symptom'))\n","                else:\n","                    entity_set.append((words[i], 'I-symptom'))\n","                done.append(words[i])\n","                \n","    words = text.split(' ')\n","    for i in range(len(words)):\n","        if words[i] not in done:\n","            entity_set.append((words[i], 'O'))\n","            \n","    # reorder list \n","    final_output = []\n","    for word in words:\n","        for item in entity_set:\n","            if word == item[0]:\n","                final_output.append(item)\n","                break\n","    \n","    return final_output\n","\n","def evaluate(y_true, y_pred, verbose):\n","    \n","    a = []\n","    b = []\n","    for tple in y_true:\n","        a.append(tple[1])\n","        \n","    for tple in y_pred:\n","        b.append(tple[1])\n","    \n","    if verbose:\n","        print(\"Accuracy: \", round(accuracy_score([a], [b]), 2))\n","        print(\"===\\n\")\n","        \n","    return round(accuracy_score([a], [b]), 2)\n","    \n","    \n","print('done')"]},{"cell_type":"markdown","metadata":{},"source":["### Exploring the Dataset\n","**It's necessary as we want to find out the appropiate data points that are suitable to train our model**"]},{"cell_type":"code","execution_count":5,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-11-07T03:31:08.948378Z","iopub.status.busy":"2023-11-07T03:31:08.947948Z","iopub.status.idle":"2023-11-07T03:31:09.415440Z","shell.execute_reply":"2023-11-07T03:31:09.414261Z","shell.execute_reply.started":"2023-11-07T03:31:08.948345Z"},"trusted":true},"outputs":[],"source":["# finding the counts of each of transcription types\n","ts_type = get_transcription_types()\n","ts_type_count = {}\n","\n","for item in ts_type:\n","    ts_type_count[item] = 0\n","        \n","for idx, ndf in df.iterrows():\n","    data = ndf['transcription']\n","    initial = get_initials(data)\n","    if initial is not None:\n","        ts_type_count[str(initial)] += 1\n","\n","sorted_ts_count = dict(sorted(ts_type_count.items(), key=lambda item: item[0], reverse=False))\n","# for ts, cnt in sorted_ts_count.items():\n","#     print(ts, cnt)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:31:09.416993Z","iopub.status.busy":"2023-11-07T03:31:09.416679Z","iopub.status.idle":"2023-11-07T03:31:09.824099Z","shell.execute_reply":"2023-11-07T03:31:09.823050Z","shell.execute_reply.started":"2023-11-07T03:31:09.416966Z"},"trusted":true},"outputs":[],"source":["with open('complain.txt', 'w+') as f:\n","    # save only the subjective transciptions\n","    for idx, ndf in df.iterrows():\n","        data = ndf['transcription']\n","        initial = get_initials(data)\n","        if initial == 'SUBJECTIVE':\n","            transcript = ndf['transcription']\n","            parts = transcript.split(':,')\n","#             print(transcript)\n","            f.write(transcript)\n","            f.write('\\n\\n')\n","\n","#             print('>>', end=' ')\n","#             print(ndf['keywords'])\n","#             print('--\\n')\n","            \n","    f.close"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset Maker Function for Training with Transformers"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:31:09.826576Z","iopub.status.busy":"2023-11-07T03:31:09.825774Z","iopub.status.idle":"2023-11-07T03:31:09.853156Z","shell.execute_reply":"2023-11-07T03:31:09.851860Z","shell.execute_reply.started":"2023-11-07T03:31:09.826544Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["class NERDataMaker:\n","    def __init__(self, texts):\n","        self.unique_entities = []\n","        self.processed_texts = []\n","\n","        temp_processed_texts = []\n","        for text in texts:\n","            tokens_with_entities = get_tokens_with_entities(text)\n","            for _, ent in tokens_with_entities:\n","                if ent not in self.unique_entities:\n","                    self.unique_entities.append(ent)\n","            temp_processed_texts.append(tokens_with_entities)\n","\n","        self.unique_entities.sort(key=lambda ent: ent if ent != \"O\" else \"\")\n","\n","        for tokens_with_entities in temp_processed_texts:\n","            self.processed_texts.append([(t, self.unique_entities.index(ent)) for t, ent in tokens_with_entities])\n","\n","    @property\n","    def id2label(self):\n","        return dict(enumerate(self.unique_entities))\n","\n","    @property\n","    def label2id(self):\n","        return {v:k for k, v in self.id2label.items()}\n","\n","    def __len__(self):\n","        return len(self.processed_texts)\n","\n","    def __getitem__(self, idx):\n","        def _process_tokens_for_one_text(id, tokens_with_encoded_entities):\n","            ner_tags = []\n","            tokens = []\n","            for t, ent in tokens_with_encoded_entities:\n","                ner_tags.append(ent)\n","                tokens.append(t)\n","\n","            return {\n","                \"id\": id,\n","                \"ner_tags\": ner_tags,\n","                \"tokens\": tokens\n","            }\n","\n","        tokens_with_encoded_entities = self.processed_texts[idx]\n","        if isinstance(idx, int):\n","            return _process_tokens_for_one_text(idx, tokens_with_encoded_entities)\n","        else:\n","            return [_process_tokens_for_one_text(i+idx.start, tee) for i, tee in enumerate(tokens_with_encoded_entities)]\n","\n","    def as_hf_dataset(self, tokenizer):\n","        \n","        def tokenize_and_align_labels(examples):\n","            tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n","\n","            labels = []\n","            for i, label in enumerate(examples[f\"ner_tags\"]):\n","                word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n","                previous_word_idx = None\n","                label_ids = []\n","                for word_idx in word_ids:  # Set the special tokens to -100.\n","                    if word_idx is None:\n","                        label_ids.append(-100)\n","                    elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n","                        label_ids.append(label[word_idx])\n","                    else:\n","                        label_ids.append(-100)\n","                    previous_word_idx = word_idx\n","                labels.append(label_ids)\n","\n","            tokenized_inputs[\"labels\"] = labels\n","            return tokenized_inputs\n","\n","        ids, ner_tags, tokens = [], [], []\n","        for i, pt in enumerate(self.processed_texts):\n","            ids.append(i)\n","            pt_tokens,pt_tags = list(zip(*pt))\n","            ner_tags.append(pt_tags)\n","            tokens.append(pt_tokens)\n","        data = {\n","            \"id\": ids,\n","            \"ner_tags\": ner_tags,\n","            \"tokens\": tokens\n","        }\n","        features = Features({\n","            \"tokens\": Sequence(Value(\"string\")),\n","            \"ner_tags\": Sequence(ClassLabel(names=self.unique_entities)),\n","            \"id\": Value(\"int32\")\n","        })\n","        print(\"Features: \", features)\n","        ds = Dataset.from_dict(data, features)\n","        tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)\n","        return tokenized_ds\n","    \n","print('done')"]},{"cell_type":"markdown","metadata":{},"source":["### Reading Data "]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:31:09.855138Z","iopub.status.busy":"2023-11-07T03:31:09.854811Z","iopub.status.idle":"2023-11-07T03:31:09.867878Z","shell.execute_reply":"2023-11-07T03:31:09.866204Z","shell.execute_reply.started":"2023-11-07T03:31:09.855111Z"},"trusted":true},"outputs":[],"source":["data = \"\"\n","with open('../datasets/annotated.txt', 'r') as f:\n","    lines = f.readlines()\n","    for line in lines:\n","        data += line"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:31:09.869757Z","iopub.status.busy":"2023-11-07T03:31:09.869337Z","iopub.status.idle":"2023-11-07T03:31:09.894118Z","shell.execute_reply":"2023-11-07T03:31:09.892994Z","shell.execute_reply.started":"2023-11-07T03:31:09.869727Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["128\n","102 26\n"]}],"source":["import random\n","\n","temp = \"\"\"1. This 35-year-old male patient comes in with complaints of [headache](symptom) and [nausea](symptom) after a recent head injury.\n","2. A 28-year-old female reports [shortness of breath](symptom), [chest pain](symptom), and [palpitations](symptom) during physical activity.\n","3. The 42-year-old patient describes [fever](symptom), [cough](symptom), and [fatigue](symptom) as their main symptoms.\n","4. This 19-year-old student presents with [fever](symptom), [sore throat](symptom), and [loss of taste and smell](symptom) for the past few days.\n","5. A 50-year-old male patient experiences [joint pain](symptom), [muscle weakness](symptom), and [fatigue](symptom).\n","6. The 31-year-old athlete complains of [knee pain](symptom), [swelling](symptom), and [limited range of motion](symptom) after a sports injury.\n","7. This 60-year-old woman reports [back pain](symptom), [numbness in legs](symptom), and [weakness](symptom) in her lower limbs.\n","8. A 25-year-old individual presents with [abdominal pain](symptom), [bloating](symptom), and [constipation](symptom).\n","9. The 48-year-old patient describes [vision problems](symptom), [headache](symptom), and [dizziness](symptom) for the past week.\n","10. This 22-year-old male reports [skin rash](symptom), [itching](symptom), and [redness](symptom) on various parts of the body.\"\"\"\n","\n","lines = data.strip().split('\\n')\n","random.shuffle(lines)\n","print(len(lines))\n","\n","train_ratio = 0.8  # 80% for training, 20% for testing\n","\n","# Calculate the split point based on the ratio\n","split_point = int(len(lines) * train_ratio)\n","\n","train_lines = lines[:split_point]\n","test_lines = lines[split_point:]\n","\n","train_text = '\\n'.join(train_lines)\n","test_text = '\\n'.join(test_lines)\n","\n","train_dm = NERDataMaker(train_text.split('\\n'))\n","test_dm = NERDataMaker(test_text.split('\\n'))\n","\n","print(len(train_dm), len(test_dm))"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:31:09.897441Z","iopub.status.busy":"2023-11-07T03:31:09.897110Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['O', 'B-symptom', 'I-symptom']\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Features:  {'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-symptom', 'I-symptom'], id=None), length=-1, id=None), 'id': Value(dtype='int32', id=None)}\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102/102 [00:00<00:00, 5407.29 examples/s]\n"]},{"name":"stdout","output_type":"stream","text":["Features:  {'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-symptom', 'I-symptom'], id=None), length=-1, id=None), 'id': Value(dtype='int32', id=None)}\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:00<00:00, 3885.28 examples/s]\n"]},{"name":"stdout","output_type":"stream","text":["{'id': 0, 'ner_tags': [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['She', 'also', 'experiences', 'vertigo', 'and', 'lightheadedness', 'but', 'does', 'not', 'have', 'these', 'symptoms', 'presently.'], 'input_ids': [101, 2016, 2036, 6322, 28246, 1998, 2422, 4974, 2098, 2791, 2021, 2515, 2025, 2031, 2122, 8030, 12825, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, 0, 1, 0, 1, -100, -100, -100, 0, 0, 0, 0, 0, 0, 0, -100, -100]}\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/77 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","                                              \n","  9%|â–‰         | 7/77 [00:13<01:57,  1.68s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.7826511859893799, 'eval_runtime': 0.635, 'eval_samples_per_second': 40.946, 'eval_steps_per_second': 3.15, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["                                               \n"," 18%|â–ˆâ–Š        | 14/77 [00:29<02:11,  2.09s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.6918734908103943, 'eval_runtime': 1.8731, 'eval_samples_per_second': 13.881, 'eval_steps_per_second': 1.068, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":["                                               \n"," 27%|â–ˆâ–ˆâ–‹       | 21/77 [00:40<01:30,  1.61s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.5975918173789978, 'eval_runtime': 0.6101, 'eval_samples_per_second': 42.615, 'eval_steps_per_second': 3.278, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":["                                               \n"," 36%|â–ˆâ–ˆâ–ˆâ–‹      | 28/77 [00:51<01:08,  1.41s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.5262717008590698, 'eval_runtime': 0.5407, 'eval_samples_per_second': 48.087, 'eval_steps_per_second': 3.699, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":["                                               \n"," 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 35/77 [01:02<00:54,  1.30s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.4824139475822449, 'eval_runtime': 0.5364, 'eval_samples_per_second': 48.47, 'eval_steps_per_second': 3.728, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["                                               \n"," 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 42/77 [01:13<00:49,  1.40s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.44765955209732056, 'eval_runtime': 0.5908, 'eval_samples_per_second': 44.007, 'eval_steps_per_second': 3.385, 'epoch': 6.0}\n"]},{"name":"stderr","output_type":"stream","text":["                                               \n"," 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 49/77 [01:25<00:40,  1.45s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.4399275779724121, 'eval_runtime': 0.6226, 'eval_samples_per_second': 41.761, 'eval_steps_per_second': 3.212, 'epoch': 7.0}\n"]},{"name":"stderr","output_type":"stream","text":["                                               \n"," 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 56/77 [01:39<00:36,  1.73s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.4371403455734253, 'eval_runtime': 0.5504, 'eval_samples_per_second': 47.234, 'eval_steps_per_second': 3.633, 'epoch': 8.0}\n"]},{"name":"stderr","output_type":"stream","text":["                                               \n"," 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 63/77 [01:49<00:19,  1.40s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.4178934693336487, 'eval_runtime': 0.6113, 'eval_samples_per_second': 42.53, 'eval_steps_per_second': 3.272, 'epoch': 9.0}\n"]},{"name":"stderr","output_type":"stream","text":["                                               \n"," 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 70/77 [02:00<00:09,  1.35s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.41518300771713257, 'eval_runtime': 0.5037, 'eval_samples_per_second': 51.613, 'eval_steps_per_second': 3.97, 'epoch': 10.0}\n"]},{"name":"stderr","output_type":"stream","text":["                                               \n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [02:12<00:00,  1.72s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.41558510065078735, 'eval_runtime': 0.6527, 'eval_samples_per_second': 39.835, 'eval_steps_per_second': 3.064, 'epoch': 11.0}\n","{'train_runtime': 132.101, 'train_samples_per_second': 8.494, 'train_steps_per_second': 0.583, 'train_loss': 0.49372541749632204, 'epoch': 11.0}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/plain":["TrainOutput(global_step=77, training_loss=0.49372541749632204, metrics={'train_runtime': 132.101, 'train_samples_per_second': 8.494, 'train_steps_per_second': 0.583, 'train_loss': 0.49372541749632204, 'epoch': 11.0})"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n","\n","print(train_dm.unique_entities)\n","\n","model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(train_dm.unique_entities), id2label=train_dm.id2label, label2id=train_dm.label2id)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,             # keep it 2e-5\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=11,\n","    weight_decay=0.01,\n",")\n","\n","train_ds = train_dm.as_hf_dataset(tokenizer=tokenizer)\n","test_ds = test_dm.as_hf_dataset(tokenizer=tokenizer)\n","\n","print(train_ds[0])\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_ds,\n","    eval_dataset=test_ds, # eval on training set! ONLY for DEMO!! have to split the data into train test split\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_9028/2330625334.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n","  metric = load_metric(\"accuracy\")\n","Downloading builder script: 4.21kB [00:00, 6.47MB/s]                   \n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.59it/s]"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.416\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from datasets import load_metric\n","metric = load_metric(\"accuracy\")\n","\n","# Evaluate the model on the test dataset\n","results = trainer.evaluate()\n","\n","# Calculate accuracy\n","accuracy = results\n","print(f\"Validation Loss: {round(accuracy['eval_loss'], 3)}\")"]},{"cell_type":"code","execution_count":20,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Average Accuracy:  0.809\n"]}],"source":["# Evaluating the model\n","from transformers import pipeline\n","\n","pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\") # pass device=0 if using gpu\n","\n","test_data = test_text.split('\\n')\n","avg_acc = []\n","for data in test_data:\n","    \n","    y_true = get_tokens_with_entities(data)\n","    \n","    data = remove_symptom_notation(data)\n","    predicted_text = pipe(data)\n","    \n","    y_pred = set_token_entities(data, predicted_text)\n","    \n","    avg_acc.append(evaluate(y_true, y_pred, False))\n","    \n","print(\"Average Accuracy: \", round(sum(avg_acc)/len(avg_acc), 3))"]},{"cell_type":"code","execution_count":25,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Symptom:  higher\n","Symptom:  lower temperature\n","Symptom:  raised\n","Symptom:  lowered blood pressure\n","Symptom:  abnormal\n"]}],"source":["from transformers import pipeline\n","\n","pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\") # pass device=0 if using gpu\n","data = 'A sign for example may be a higher or lower temperature than normal, raised or lowered blood pressure or an abnormality showing on a medical scan.'\n","data = remove_symptom_notation(data)\n","predicted_text = pipe(data)\n","for i in range(len(predicted_text)):\n","    print('Symptom: ', predicted_text[i]['word'])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
